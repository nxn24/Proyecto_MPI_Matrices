# Proyecto 2 â€” MultiplicaciÃ³n de Matrices Paralela con MPI
**ComputaciÃ³n de Alto Rendimiento (HPC) â€” 2025**
*Universidad Nacional Mayor de San Marcos*


---


## ğŸ‘¥ Integrantes


| CÃ³digo   | Apellidos y Nombres        | Email                        |
|----------|----------------------------|------------------------------|
| 20200053 | SÃ¡nchez Villalta, Nixon    | nixon.sanchez@unmsm.edu.pe   |
| 22200136 | Ttito Carhuas, Carolhay    | carolhay.ttito@unmsm.edu.pe  |
| 21200085 | Vilca GarcÃ­a, Wendy        | wendy.vilca@unmsm.edu.pe     |


---


## ğŸ“‹ 1. Resumen (Abstract)


Este proyecto implementa y analiza la multiplicaciÃ³n de matrices densas mediante una estrategia de paralelizaciÃ³n basada en *striping* por filas utilizando **OpenMPI** en un entorno de **memoria distribuida**.


Durante la **Semana 1** se desarrollÃ³ la implementaciÃ³n secuencial, la modularizaciÃ³n del cÃ³digo, el `Makefile` y la configuraciÃ³n inicial de MPI.
En semanas posteriores se evaluarÃ¡n *speedup*, *eficiencia* y *sobrecarga de comunicaciÃ³n*, empleando `MPI_Scatter`, `MPI_Bcast` y `MPI_Gather`.


---


## ğŸ¯ 2. IntroducciÃ³n


La multiplicaciÃ³n de matrices es un componente esencial en:


- Simulaciones fÃ­sicas
- Ãlgebra lineal numÃ©rica
- Machine Learning
- Procesamiento numÃ©rico


Debido a su costo $\mathcal{O}(N^3)$, la paralelizaciÃ³n se vuelve necesaria para tamaÃ±os grandes.
Siguiendo a **Dongarra et al. (2022)**, la optimizaciÃ³n de operaciones de Ã¡lgebra lineal continÃºa siendo un reto en HPC.
Este proyecto se basa tambiÃ©n en lineamientos de **Gropp et al. (2021)**.


---


## ğŸ“˜ 3. Fundamento TeÃ³rico


### 3.1. Algoritmo de MultiplicaciÃ³n


$$
C[i][j] = \sum_{k=0}^{N-1} A[i][k] \cdot B[k][j]
$$


**Complejidad:** $\mathcal{O}(N^3)$


### 3.2. Modelo de ComunicaciÃ³n MPI


| Comando      | Rol                                      |
|--------------|------------------------------------------|
| `MPI_Scatter`| Distribuye filas de $A$                  |
| `MPI_Bcast`  | Transmite la matriz $B$ completa         |
| `MPI_Gather` | Recolecta los bloques parciales de $C$   |


---


## ğŸ—ï¸ 4. DiseÃ±o e ImplementaciÃ³n


### 4.1. DescomposiciÃ³n


$A$ se divide por filas entre los procesos:


$$
A = [A_0, A_1, \dots, A_{p-1}]
$$


Cada proceso calcula:
$$
C_i = A_i \times B
$$


### 4.2. Flujo de Control


ğŸ”¹ **Proceso RaÃ­z (rank 0)**
- Genera matrices $A$ y $B$
- Distribuye $A$ (`MPI_Scatter`)
- Difunde $B$ (`MPI_Bcast`)
- Recolecta $C$ (`MPI_Gather`)
- Valida resultados con tolerancia $10^{-9}$


ğŸ”¹ **Workers**
- Reciben su porciÃ³n $A_i$
- Calculan $C_i$
- EnvÃ­an resultados al root


---


## ğŸ§± 5. Estructura del Proyecto â€” Semana 1 (Avanzada)


```
Proyecto_MPI_Matrices/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.c         # Punto de entrada + MPI_Init
â”‚   â”œâ”€â”€ matrix_ops.h   # Prototipos
â”‚   â””â”€â”€ matrix_ops.c   # ImplementaciÃ³n secuencial
â”œâ”€â”€ Makefile           # CompilaciÃ³n automÃ¡tica
â”œâ”€â”€ README.md          # Este archivo
â””â”€â”€ .gitignore         # ExclusiÃ³n de binarios
```


---


## âš™ï¸ 6. CompilaciÃ³n y EjecuciÃ³n


### 6.1. CompilaciÃ³n
```bash
make
```
O manualmente:
```bash
mpicc -Wall -Wextra -O2 -std=c11 -lm -o multiplicar_matrices src/main.c src/matrix_ops.c
```


### 6.2. EjecuciÃ³n
```bash
mpirun -np 4 ./multiplicar_matrices 4
mpirun -np 4 ./multiplicar_matrices 8
```


---


## ğŸ“Š 7. Resultados Esperados (Semanas 2â€“3)


| MÃ©trica     | FÃ³rmula                          | Utilidad                    |
|-------------|----------------------------------|-----------------------------|
| **Speedup** | $S(p) = \dfrac{T_1}{T_p}$       | AceleraciÃ³n general         |
| **Eficiencia** | $E(p) = \dfrac{S(p)}{p}$      | Uso del paralelismo         |
| **Overhead** | $T_p = T_{\text{comp}} + T_{\text{comm}}$ | Costo de comunicaciÃ³n |


---


## ğŸ§ª 8. Avances â€” Semana 1


âœ… ImplementaciÃ³n secuencial
âœ… GeneraciÃ³n aleatoria de matrices
âœ… ValidaciÃ³n numÃ©rica ($10^{-9}$)
âœ… ModularizaciÃ³n (`src/`, headers)
âœ… ConfiguraciÃ³n inicial MPI
âœ… MediciÃ³n con `MPI_Wtime`
âœ… Makefile funcional
âœ… README elaborado y completo


---


## ğŸ§­ 9. Conclusiones y Trabajo Futuro


### Conclusiones
- La versiÃ³n secuencial sirve como *baseline* confiable.
- La modularizaciÃ³n facilita el desarrollo paralelo.
- MPI es adecuado para memoria distribuida y escalamiento.


### Trabajo Futuro
- ImplementaciÃ³n paralela completa
- VersiÃ³n por bloques
- Tipos derivados MPI
- AnÃ¡lisis de escalabilidad fuerte y dÃ©bil


---


## ğŸ“š Referencias


1. Dongarra et al. (2022). *The Singular Value Decomposition: A Tour of Its Historical Development and Current State-of-the-Art*.
2. Gropp, Lusk & Thakur (2021). *Using MPI-3: Portable Parallel Programming with the Message-Passing Interface*. MIT Press.
3. Demmel & Hoemmen (2023). *Communication-Avoiding Numerical Linear Algebra*.
