# Proyecto 2 ‚Äî Multiplicaci√≥n de Matrices Paralela con MPI
**Computaci√≥n de Alto Rendimiento (HPC) ‚Äî 2025**  
*Universidad Nacional Mayor de San Marcos*

---

## üë• Integrantes

| C√≥digo   | Apellidos y Nombres        | Email                        |
|----------|----------------------------|------------------------------|
| 20200053 | S√°nchez Villalta, Nixon    | nixon.sanchez@unmsm.edu.pe   |
| 22200136 | Ttito Carhuas, Carolhay    | carolhay.ttito@unmsm.edu.pe  |
| 21200085 | Vilca Garc√≠a, Wendy        | wendy.vilca@unmsm.edu.pe     |

---

## üìã 1. Resumen (Abstract)

Este proyecto implementa, analiza y compara distintos enfoques de multiplicaci√≥n de matrices densas utilizando **OpenMPI** bajo el modelo de **memoria distribuida**.

- En la **Semana 1**, se construy√≥ la versi√≥n secuencial, la validaci√≥n num√©rica y la estructura modular del proyecto.
- En la **Semana 2**, se desarrollaron **dos estrategias de paralelizaci√≥n**:
    - **MPI_Scatterv + MPI_Bcast + MPI_Gatherv** ‚Üí estrategia principal del laboratorio.
    - **MPI_Bcast + MPI_Reduce** ‚Üí estrategia adicional para an√°lisis comparativo.

La Semana 3 se centrar√° en mediciones completas de rendimiento: tiempo, speedup, eficiencia y comportamiento de escalabilidad.

---

## üéØ 2. Introducci√≥n

La multiplicaci√≥n de matrices es esencial en:

- Simulaciones f√≠sicas y modelos matem√°ticos
- M√©todos num√©ricos
- Ciencia de datos y machine learning
- Operaciones fundamentales de √°lgebra lineal

Al tener una complejidad c√∫bica:
\[
\mathcal{O}(N^3)
\]
la paralelizaci√≥n es necesaria para manejar tama√±os grandes de matriz.

El proyecto explora t√©cnicas de **descomposici√≥n de datos** y **comunicaci√≥n entre procesos** usando MPI, siguiendo buenas pr√°cticas propuestas por Gropp, Lusk, Thakur y Dongarra.

---

## üìò 3. Fundamento Te√≥rico

### 3.1. Algoritmo de Multiplicaci√≥n Matricial

\[
C[i][j] = \sum_{k=0}^{N-1} A[i][k] \cdot B[k][j]
\]

El algoritmo requiere:
- \(N^3\) multiplicaciones
- \(N^3 - N^2\) sumas

---

### 3.2. Modelo de Comunicaci√≥n MPI

| Operaci√≥n MPI | Funci√≥n en el algoritmo                             |
|---------------|------------------------------------------------------|
| `MPI_Scatterv`| Distribuci√≥n de filas de A (bloques irregulares)     |
| `MPI_Bcast`   | Difusi√≥n completa de la matriz B                     |
| `MPI_Gatherv` | Recolecci√≥n ordenada de los bloques parciales de C   |
| `MPI_Reduce`  | Combinaci√≥n de resultados entre procesos             |

---

## üèóÔ∏è 4. Dise√±o e Implementaci√≥n

### 4.1. Descomposici√≥n por Filas

La matriz A se divide entre los procesos de forma balanceada:

\[
A = [A_0, A_1, \dots, A_{p-1}]
\]

Cada proceso calcula:

\[
C_i = A_i \cdot B
\]

---

## 4.2 Estrategia 1 ‚Äî **MPI_Scatterv + MPI_Bcast + MPI_Gatherv**

### Flujo:
1. **Distribuci√≥n:**  
   `MPI_Scatterv` reparte porciones de \(A\), incluso si \(N \mod p \neq 0\).

2. **Difusi√≥n:**  
   `MPI_Bcast` env√≠a la matriz \(B\) completa a todos los procesos.

3. **C√°lculo local:**  
   Cada proceso computa su bloque \(C_i\).

4. **Recolecci√≥n:**  
   `MPI_Gatherv` reconstruye la matriz final \(C\) en el proceso root.

### Ventajas:
- Distribuci√≥n balanceada incluso en casos irregulares.
- Excelente para el patr√≥n striping por filas.
- Escalabilidad adecuada en entornos distribuidos.

---

## 4.3 Estrategia 2 ‚Äî **MPI_Bcast + MPI_Reduce** (adicional)

### Flujo:
1. `MPI_Bcast` difunde A y B completas.
2. Cada proceso calcula un subconjunto de filas.
3. `MPI_Reduce` combina los resultados finales.

### Pros y Contras:
| Ventaja              | Desventaja                          |
|----------------------|-------------------------------------|
| F√°cil de implementar | Toma m√°s memoria                    |
| Buena para pruebas   | M√°s comunicaci√≥n que Scatter/Gather |

Se usa para comparaci√≥n y an√°lisis.

---


## üß± 5. Estructura del Proyecto ‚Äî Semana 2 


```
Proyecto_MPI_Matrices/
‚îú‚îÄ‚îÄ src/
‚îÇ ‚îú‚îÄ‚îÄ main.c # Control del programa + rendimiento
‚îÇ ‚îú‚îÄ‚îÄ matrix_ops.h # Funciones secuenciales
‚îÇ ‚îú‚îÄ‚îÄ matrix_ops.c # Multiplicaci√≥n secuencial
‚îÇ ‚îú‚îÄ‚îÄ mpi_ops.h # Funciones MPI
‚îÇ ‚îî‚îÄ‚îÄ mpi_ops.c # Implementaci√≥n Scatter/Bcast/Gather + Reduce
‚îú‚îÄ‚îÄ Makefile
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ .gitignore
```


---


## ‚öôÔ∏è 6. Compilaci√≥n y Ejecuci√≥n


### 6.1. Compilaci√≥n
```bash
make clean && make
```


### 6.2. Ejecuci√≥n
```bash
mpirun -np 4 ./matrix_multiply 4
mpirun -np 8 ./matrix_multiply 512
mpirun --oversubscribe -np 8 ./matrix_multiply 1024
```


---


## üìä 7. Resultados de Correctitud ‚Äî Semana 2

En esta etapa del proyecto se verific√≥ lo siguiente:

- ‚úî **Correspondencia entre los resultados secuenciales y paralelos**, utilizando ambas estrategias MPI implementadas.
- ‚úî **Tolerancia de comparaci√≥n num√©rica:**  
  \[
  1 \times 10^{-9}
  \]
- ‚úî **Validaci√≥n de funcionalidad en diferentes tama√±os:**  
  \[
  N = 64,\ 128,\ 256,\ 512
  \]

> La medici√≥n de **speedup**, **eficiencia** y **escalabilidad** se realizar√° en la **Semana 3**, una vez completada la fase final del laboratorio.

---

## üß™ 8. Avances por Semana

### ‚úî Semana 1
- Implementaci√≥n secuencial completa.
- Modularizaci√≥n del proyecto en `src/`.
- Generaci√≥n y liberaci√≥n segura de matrices.
- Verificaci√≥n num√©rica con tolerancia estricta.
- `Makefile` funcional y automatizado.
- Configuraci√≥n inicial del entorno MPI.

### ‚úî Semana 2
- Implementaci√≥n completa de la estrategia **Scatter/Bcast/Gatherv**.
- Implementaci√≥n adicional de la estrategia **Broadcast + Reduce**.
- Manejo seguro de procesos sin carga (`filas_local = 0`).
- Redise√±o del `main.c` para soportar pruebas paralelas y validaciones.
- Implementaci√≥n de funciones de rendimiento con `MPI_Wtime`.
- Comentarios a√±adidos en las funciones clave del c√≥digo.
- Actualizaci√≥n integral del README con teor√≠a y estructura.

---

## üß≠ 9. Conclusiones y Trabajo Futuro

### Conclusiones
- **`MPI_Scatterv`** permite una distribuci√≥n eficiente incluso con matrices que no se dividen equitativamente entre los procesos.
- Las dos estrategias paralelas implementadas (**Scatter/Gather** y **Broadcast/Reduce**) producen resultados correctos y verificables.
- Se estableci√≥ una base s√≥lida para realizar an√°lisis de rendimiento en la pr√≥xima semana.

### Trabajo Futuro
- Comparaci√≥n detallada de **speedup** y **eficiencia**.
- Evaluaci√≥n de **escalabilidad fuerte y d√©bil**.
- Implementaci√≥n de la multiplicaci√≥n **por bloques**.
- Uso de **tipos derivados MPI** para optimizar a√∫n m√°s la comunicaci√≥n.

---

## üìö Referencias

1. Dongarra et al. (2022). *The Singular Value Decomposition: A Tour of Its Historical Development and Current State-of-the-Art*.
2. Gropp, Lusk & Thakur (2021). *Using MPI-3: Portable Parallel Programming with the Message-Passing Interface*. MIT Press.
3. Demmel & Hoemmen (2023). *Communication-Avoiding Numerical Linear Algebra*.

